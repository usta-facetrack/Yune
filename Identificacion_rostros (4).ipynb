{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DATATÓN ACADÉMICO**\n",
        "\n",
        "## **Reconocimiento Facial para registro de asistencia**\n",
        "\n",
        "**Estudiante:** Yuneidy Lorena Gutierrez Diaz\n",
        "\n",
        "**Espacio Académico:** Redes Neuronales\n",
        "\n",
        "\n",
        "### **Objetivo General**\n",
        "\n",
        "Diseñar mejoras al pipeline de reconocimiento facial implementado para registrar la asistencia de estudiantes. El reto\n",
        "se enfoca en aumentar la precisión del modelo y mejorar su rendimiento bajo condiciones reales, sin requerir interfaz\n",
        "ni despliegue.\n",
        "\n",
        "## **Contexto Técnico**\n",
        "Actualmente se utiliza un modelo funcional basado en detección de rostros con MTCNN, generación de embeddings\n",
        "faciales con FaceNet, y comparación mediante distancias para determinar la identidad de los estudiantes. Este\n",
        "sistema ha sido útil, pero requiere mejoras para lograr mayor robustez y precisión.\n"
      ],
      "metadata": {
        "id": "QKcAMnrpoiDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch torch torchvision numpy pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fkVKShxepiMo",
        "outputId": "07d0d59e-fc2a-40d9-e3cb-4a378c695af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iud5tv3pi8N",
        "outputId": "d47f9b7c-2c2b-4f3c-9071-667fa038cc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "import torch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "3I-o-xLhp0yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuración FaceNet ---\n",
        "# Configurar dispositivo (GPU si está disponible)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Inicializar detector y modelo de embeddings\n",
        "mtcnn = MTCNN(image_size=160, margin=20, min_face_size=20, device=device)\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n"
      ],
      "metadata": {
        "id": "TNKVsj1DqB0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Funciones comunes para cargar imágenes desde enlaces de Google Drive ---\n",
        "def extraer_id_enlace(enlace):\n",
        "    if \"id=\" in enlace:\n",
        "        return enlace.split(\"id=\")[-1].split(\"&\")[0]\n",
        "    elif \"file/d/\" in enlace:\n",
        "        return enlace.split(\"file/d/\")[-1].split(\"/\")[0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def cargar_imagen_desde_drive(file_id):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    imagen = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    return imagen"
      ],
      "metadata": {
        "id": "40MoVTtqqIBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Funciones de extracción de embeddings ---\n",
        "def get_facenet_embedding_from_link(link):\n",
        "    file_id = extraer_id_enlace(link)\n",
        "    if file_id is None:\n",
        "        return None\n",
        "    try:\n",
        "        img = cargar_imagen_desde_drive(file_id)\n",
        "        face_tensor = mtcnn(img)\n",
        "        if face_tensor is not None:\n",
        "            face_tensor = face_tensor.unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                emb = facenet(face_tensor)\n",
        "            return emb.cpu().numpy().flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error FaceNet con {link}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "_Ls4KtOsrmC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Funciones para FaceNet ---\n",
        "def get_facenet_embedding_from_link(link):\n",
        "    file_id = extraer_id_enlace(link)\n",
        "    if file_id is None:\n",
        "        return None\n",
        "    try:\n",
        "        img = cargar_imagen_desde_drive(file_id)\n",
        "        face_tensor = mtcnn(img)\n",
        "        if face_tensor is not None:\n",
        "            face_tensor = face_tensor.unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                emb = facenet(face_tensor)\n",
        "            return emb.cpu().numpy().flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error FaceNet con {link}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "8z2A1a3Dqcx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer archivo Excel con nombres y enlaces\n",
        "excel_path = '/content/drive/MyDrive/Fotos RN (respuestas).xlsx'  # Ajusta la ruta\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "# Crear diccionario {nombre: [lista_de_enlaces]}\n",
        "image_dict = {}\n",
        "for idx, row in df.iterrows():\n",
        "    nombre = str(row['Nombre Completo']).strip()\n",
        "    fotos = str(row['Ingrese sus fotos']).split(',')\n",
        "    fotos = [f.strip() for f in fotos if f.strip() != '']\n",
        "    if nombre not in image_dict:\n",
        "        image_dict[nombre] = []\n",
        "    image_dict[nombre].extend(fotos)\n",
        "\n",
        "# División train/test 70/30\n",
        "def split_image_dict(image_dict, test_size=0.3, random_state=42):\n",
        "    train_dict, test_dict = {}, {}\n",
        "    for name, links in image_dict.items():\n",
        "        if len(links) < 2:\n",
        "            continue  # mínimo 2 imágenes para dividir\n",
        "        train_links, test_links = train_test_split(links, test_size=test_size, random_state=random_state)\n",
        "        train_dict[name] = train_links\n",
        "        test_dict[name] = test_links\n",
        "    return train_dict, test_dict\n",
        "\n",
        "train_dict, test_dict = split_image_dict(image_dict)"
      ],
      "metadata": {
        "id": "_5mpsFpPsEcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construir base de embeddings\n",
        "def build_embeddings_db_from_links(image_dict, embedding_fn):\n",
        "    db = {}\n",
        "    for name, links in image_dict.items():\n",
        "        embeddings = []\n",
        "        for link in links:\n",
        "            emb = embedding_fn(link)\n",
        "            if emb is not None:\n",
        "                embeddings.append(emb)\n",
        "        if embeddings:\n",
        "            db[name] = embeddings\n",
        "    return db\n",
        "\n",
        "print(\"Extrayendo embeddings FaceNet\")\n",
        "facenet_train_db = build_embeddings_db_from_links(train_dict, get_facenet_embedding_from_link)\n",
        "facenet_test_db = build_embeddings_db_from_links(test_dict, get_facenet_embedding_from_link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpMKzwcAsPvL",
        "outputId": "848b6415-a1cb-41dc-bc6e-29a4f5549c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extrayendo embeddings FaceNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación con métricas\n",
        "def recognize_and_evaluate(test_db, train_db):\n",
        "    y_true, y_pred = [], []\n",
        "    centroids = {name: np.mean(embs, axis=0) for name, embs in train_db.items()}\n",
        "    for true_name, test_embs in test_db.items():\n",
        "        for emb in test_embs:\n",
        "            sims = {name: cosine_similarity([emb], [centroid])[0][0] for name, centroid in centroids.items()}\n",
        "            pred_name = max(sims, key=sims.get)\n",
        "            y_true.append(true_name)\n",
        "            y_pred.append(pred_name)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    return acc, prec, rec\n",
        "\n",
        "print(\"Evaluando FaceNet...\")\n",
        "acc, prec, rec = recognize_and_evaluate(facenet_test_db, facenet_train_db)\n",
        "print(f\"FaceNet - Accuracy: {acc:.4f}, Precisión: {prec:.4f}, Recall: {rec:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjrdU7jnsRy2",
        "outputId": "495eb8c8-48b6-435c-fc4f-45a502793460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluando FaceNet...\n",
            "FaceNet - Accuracy: 0.9000, Precisión: 0.8750, Recall: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretación de Resultados - FaceNet**\n",
        "* Accuracy: 0.9000\n",
        "\n",
        "FaceNet identificó correctamente el 90.00% de los rostros en el conjunto de prueba. Esto sugiere que FaceNet tiene una  capacidad para reconocer rostros en general en este conjunto de datos específico.\n",
        "\n",
        "* Precisión: 0.8750\n",
        "\n",
        "De todas las identificaciones que FaceNet hizo como positivas, el 87.50% fueron realmente correctas. Esto implica que FaceNet tiene una tasa de falsos positivos, lo que puede ser valioso en aplicaciones donde la precisión es crítica.\n",
        "\n",
        "* Recall: 0.9000\n",
        "\n",
        "FaceNet fue capaz de identificar correctamente el 90.00% de todos los rostros que realmente correspondían a una persona. Esto sugiere que FaceNet tiene una mayor capacidad para encontrar la mayoría de los rostros relevantes, minimizando aún más los falsos negativos.\n",
        "\n",
        "Finalmente FaceNet muestra un rendimiento ligeramente superior en términos de exactitud y exhaustividad. Tiene una buena capacidad para reconocer rostros en general, con una tasa de falsos positivos muy baja y una alta capacidad para encontrar la mayoría de los rostros relevantes."
      ],
      "metadata": {
        "id": "T24ZGKIgrFbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Explorar nuevos modelos o embeddings (como ArcFace o InsightFace).**"
      ],
      "metadata": {
        "id": "SWHqD_lMtRpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime insightface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P6LjQtyqI_3T",
        "outputId": "1dd3ffc0-b6c0-4d15-ee53-5ee61078b187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
            "Requirement already satisfied: insightface in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from insightface) (1.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from insightface) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from insightface) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from insightface) (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from insightface) (10.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from insightface) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from insightface) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from insightface) (0.25.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from insightface) (1.13)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from insightface) (3.0.12)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (from insightface) (2.0.7)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from insightface) (3.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (2.11.4)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (6.2.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (2.9.0.post0)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx->insightface) (4.13.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->insightface) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2025.4.26)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2025.5.21)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerias necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J434hg8UtnhS",
        "outputId": "ee2bb8c6-e1da-4d2e-d215-c30a7d233e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.7'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelo ArcFace preentrenado\n",
        "arcface_app = FaceAnalysis(name='buffalo_l')\n",
        "arcface_app.prepare(ctx_id=0)  # Usa ctx_id=-1 si no tienes GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGfqJYFptsVs",
        "outputId": "83f76731-10c4-4f59-d442-ee82253eef5f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para extraer embedding ArcFace desde enlace\n",
        "def get_arcface_embedding_from_link(link):\n",
        "    file_id = extraer_id_enlace(link)\n",
        "    if file_id is None:\n",
        "        return None\n",
        "    try:\n",
        "        img = np.array(cargar_imagen_desde_drive(file_id))\n",
        "        faces = arcface_app.get(img)\n",
        "        if len(faces) > 0:\n",
        "            return faces[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error con {link}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "oN6iaASbtuqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# División 70/30 train/test\n",
        "def split_image_dict(image_dict, test_size=0.3, random_state=42):\n",
        "    train_dict, test_dict = {}, {}\n",
        "    for name, links in image_dict.items():\n",
        "        if len(links) < 2:\n",
        "            continue  # Necesitas al menos 2 imágenes para dividir\n",
        "        train_links, test_links = train_test_split(links, test_size=test_size, random_state=random_state)\n",
        "        train_dict[name] = train_links\n",
        "        test_dict[name] = test_links\n",
        "    return train_dict, test_dict\n",
        "\n",
        "train_dict, test_dict = split_image_dict(image_dict)\n",
        "\n",
        "# Construir base de embeddings\n",
        "def build_embeddings_db_from_links(image_dict, embedding_fn):\n",
        "    db = {}\n",
        "    for name, links in image_dict.items():\n",
        "        embeddings = []\n",
        "        for link in links:\n",
        "            emb = embedding_fn(link)\n",
        "            if emb is not None:\n",
        "                embeddings.append(emb)\n",
        "        if embeddings:\n",
        "            db[name] = embeddings\n",
        "    return db\n",
        "\n",
        "print(\"Extrayendo embeddings ArcFace\")\n",
        "arcface_train_db = build_embeddings_db_from_links(train_dict, get_arcface_embedding_from_link)\n",
        "arcface_test_db = build_embeddings_db_from_links(test_dict, get_arcface_embedding_from_link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLr2XvZnt7_o",
        "outputId": "6d151d51-4049-4c52-eead-a6073bed8f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extrayendo embeddings ArcFace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluación con métricas\n",
        "def recognize_and_evaluate(test_db, train_db):\n",
        "    y_true, y_pred = [], []\n",
        "    centroids = {name: np.mean(embs, axis=0) for name, embs in train_db.items()}\n",
        "    for true_name, test_embs in test_db.items():\n",
        "        for emb in test_embs:\n",
        "            sims = {name: cosine_similarity([emb], [centroid])[0][0] for name, centroid in centroids.items()}\n",
        "            pred_name = max(sims, key=sims.get)\n",
        "            y_true.append(true_name)\n",
        "            y_pred.append(pred_name)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    return acc, prec, rec\n",
        "\n",
        "acc, prec, rec = recognize_and_evaluate(arcface_test_db, arcface_train_db)\n",
        "print(f\"ArcFace - Accuracy: {acc:.4f}, Precisión: {prec:.4f}, Recall: {rec:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXzMVoZVt8wt",
        "outputId": "25aca267-1942-4500-f07d-a95d2594463e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ArcFace - Accuracy: 0.8810, Precisión: 0.8690, Recall: 0.8810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretación de Resultados - ArcFace**\n",
        "\n",
        "* Accuracy: 0.8810\n",
        "\n",
        "Indica que ArcFace identificó correctamente el 88.10% de los rostros en el conjunto de prueba. Esto es un buen resultado y sugiere que el modelo tiene una buena capacidad para reconocer rostros en general.\n",
        "\n",
        "* Precisión: 0.8690\n",
        "\n",
        "De todas las identificaciones que ArcFace hizo como positivas, el 86.90% fueron realmente correctas. Esto implica que el modelo tiene una baja tasa de falsos positivos, lo cual es importante en aplicaciones donde es crucial evitar identificaciones incorrectas.\n",
        "\n",
        "* Recall: 0.8810\n",
        "\n",
        "ArcFace fue capaz de identificar correctamente el 88.10% de todos los rostros que realmente correspondían a una persona. Esto sugiere que el modelo tiene una buena capacidad para encontrar la mayoría de los rostros relevantes, minimizando los falsos negativos.\n",
        "\n",
        "Finalmente ArcFace muestra un rendimiento equilibrado con una buena exactitud, precisión y exhaustividad. Esto indica que es un modelo confiable para el reconocimiento facial, con una baja tasa de errores tanto en identificaciones incorrectas como en rostros no detectados. Este algoritmo utiliza redes convolucionales para generar vectores altamente discriminativos."
      ],
      "metadata": {
        "id": "fIyxqkH8rLu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparación de modelos\n",
        "\n"
      ],
      "metadata": {
        "id": "8v7B6ydXu88R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Extraer embeddings y evaluar FaceNet\n",
        "facenet_train_db = build_embeddings_db_from_links(train_dict, get_facenet_embedding_from_link)\n",
        "facenet_test_db = build_embeddings_db_from_links(test_dict, get_facenet_embedding_from_link)\n",
        "acc_fn, prec_fn, rec_fn,= recognize_and_evaluate(facenet_test_db, facenet_train_db)"
      ],
      "metadata": {
        "id": "FpOngGthJN90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recognize_and_evaluate(test_db, train_db):\n",
        "    y_true, y_pred = [], []\n",
        "    centroids = {name: np.mean(embs, axis=0) for name, embs in train_db.items()}\n",
        "    for true_name, test_embs in test_db.items():\n",
        "        for emb in test_embs:\n",
        "            sims = {name: cosine_similarity([emb], [cent])[0][0] for name, cent in centroids.items()}\n",
        "            pred_name = max(sims, key=sims.get)\n",
        "            y_true.append(true_name)\n",
        "            y_pred.append(pred_name)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    return acc, prec, rec, y_true, y_pred"
      ],
      "metadata": {
        "id": "-W48i-ReKhKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Extraer embeddings y evaluar ArcFace\n",
        "arcface_train_db = build_embeddings_db_from_links(train_dict, get_arcface_embedding_from_link)\n",
        "arcface_test_db = build_embeddings_db_from_links(test_dict, get_arcface_embedding_from_link)\n",
        "acc_af, prec_af, rec_af, _, _ = recognize_and_evaluate(arcface_test_db, arcface_train_db)\n",
        "\n",
        "# 3. Crear tabla comparativa automáticamente\n",
        "import pandas as pd\n",
        "\n",
        "df_comparacion = pd.DataFrame({\n",
        "    'Modelo': ['FaceNet', 'ArcFace'],\n",
        "    'Accuracy': [acc_fn, acc_af],\n",
        "    'Precisión': [prec_fn, prec_af],\n",
        "    'Recall': [rec_fn, rec_af]\n",
        "})\n",
        "\n",
        "print(df_comparacion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO0_-DhWvAih",
        "outputId": "667a28b0-272f-40bd-8300-bc6bda46b22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Modelo  Accuracy  Precisión    Recall\n",
            "0  FaceNet  0.900000   0.875000  0.900000\n",
            "1  ArcFace  0.880952   0.869048  0.880952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluacion de la predicción de cada modelo**"
      ],
      "metadata": {
        "id": "NuEhB2Vh07Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "def evaluar_predicciones(y_true, y_pred, nombre_modelo=\"Modelo\"):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    print(f\"Evaluación de {nombre_modelo}:\")\n",
        "    print(f\"  Accuracy:  {acc:.4f}\")\n",
        "    print(f\"  Precisión: {prec:.4f}\")\n",
        "    print(f\"  Recall:    {rec:.4f}\")\n",
        "    print(\"\\nReporte detallado por clase:\")\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "I8WJFoc_3q4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_fn, prec_fn, rec_fn, y_true_fn, y_pred_fn = recognize_and_evaluate(facenet_test_db, facenet_train_db)\n",
        "acc_af, prec_af, rec_af, y_true_af, y_pred_af = recognize_and_evaluate(arcface_test_db, arcface_train_db)"
      ],
      "metadata": {
        "id": "6ybiR7lQOF9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FaceNet"
      ],
      "metadata": {
        "id": "UNeAqwIP1eI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_predicciones(y_true_fn, y_pred_fn, nombre_modelo=\"FaceNet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u301aq5ZOIG7",
        "outputId": "4b44bae3-2b0a-41d1-83d7-76027cf98db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluación de FaceNet:\n",
            "  Accuracy:  0.9000\n",
            "  Precisión: 0.8750\n",
            "  Recall:    0.9000\n",
            "\n",
            "Reporte detallado por clase:\n",
            "                                  precision    recall  f1-score   support\n",
            "\n",
            "        Angela Rocio Rico Ortega       1.00      1.00      1.00         2\n",
            "  Angela Tatiana orjuela guevara       1.00      1.00      1.00         2\n",
            "    Camilo Andrés Castillo Riaño       1.00      1.00      1.00         2\n",
            "        Carlos Esteban Diaz Ruan       1.00      1.00      1.00         2\n",
            "     Diego Arturo Castro Beltran       1.00      1.00      1.00         2\n",
            "            Felipe Herrera Riaño       0.33      0.50      0.40         2\n",
            "  Hayder arley Rodriguez Orjuela       1.00      1.00      1.00         2\n",
            "       JUAN FELIPE HERRERA RIANO       0.00      0.00      0.00         2\n",
            "        Juan Camilo Ortiz Ibañez       1.00      1.00      1.00         2\n",
            "                   Julian Mendez       1.00      1.00      1.00         1\n",
            "     Kewin Damián gacha guayacan       0.67      1.00      0.80         2\n",
            "       Lina Manuela Rozo Clavijo       1.00      1.00      1.00         2\n",
            "   Maria Camila Infante González       1.00      1.00      1.00         2\n",
            "      Maria Jose Galindo Piraban       1.00      1.00      1.00         2\n",
            "            Maria Jose Mejia Zea       1.00      1.00      1.00         1\n",
            "Mónica Yadira Rodríguez Martínez       0.00      0.00      0.00         1\n",
            "      Paula Ximena Guevara Gomez       1.00      1.00      1.00         3\n",
            "Thomas Santiago Orjuela Martinez       1.00      1.00      1.00         2\n",
            "   Ximena Andrea Arias Contreras       1.00      1.00      1.00         2\n",
            " Yeimy Katherine Alarcón Almanza       1.00      1.00      1.00         2\n",
            "   Yuneidy Lorena Gutierrez Diaz       1.00      1.00      1.00         2\n",
            "\n",
            "                        accuracy                           0.90        40\n",
            "                       macro avg       0.86      0.88      0.87        40\n",
            "                    weighted avg       0.88      0.90      0.89        40\n",
            "\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ArceFace"
      ],
      "metadata": {
        "id": "PdeJKMpL1ky8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_predicciones(y_true_af, y_pred_af, nombre_modelo=\"ArcFace\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Npnf1XNSP6g",
        "outputId": "8b7d28ec-b82c-40aa-9cfe-31f98e38ee7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluación de ArcFace:\n",
            "  Accuracy:  0.8810\n",
            "  Precisión: 0.8690\n",
            "  Recall:    0.8810\n",
            "\n",
            "Reporte detallado por clase:\n",
            "                                  precision    recall  f1-score   support\n",
            "\n",
            "        Angela Rocio Rico Ortega       1.00      1.00      1.00         2\n",
            "  Angela Tatiana orjuela guevara       1.00      1.00      1.00         2\n",
            "    Camilo Andrés Castillo Riaño       1.00      1.00      1.00         1\n",
            "        Carlos Esteban Diaz Ruan       1.00      1.00      1.00         2\n",
            "     Diego Arturo Castro Beltran       1.00      1.00      1.00         2\n",
            "            Felipe Herrera Riaño       0.33      0.50      0.40         2\n",
            "  Hayder arley Rodriguez Orjuela       1.00      1.00      1.00         2\n",
            "       JUAN FELIPE HERRERA RIANO       0.00      0.00      0.00         2\n",
            "        Juan Camilo Ortiz Ibañez       1.00      1.00      1.00         2\n",
            "                   Julian Mendez       1.00      1.00      1.00         1\n",
            "     Kewin Damián gacha guayacan       1.00      0.67      0.80         3\n",
            "       Lina Manuela Rozo Clavijo       1.00      1.00      1.00         2\n",
            "   Maria Camila Infante González       1.00      1.00      1.00         2\n",
            "      Maria Jose Galindo Piraban       0.67      1.00      0.80         2\n",
            "            Maria Jose Mejia Zea       0.50      1.00      0.67         1\n",
            "Mónica Yadira Rodríguez Martínez       1.00      1.00      1.00         2\n",
            "      Paula Ximena Guevara Gomez       1.00      1.00      1.00         3\n",
            "          Ricardo Vargas Ramírez       1.00      1.00      1.00         2\n",
            "Thomas Santiago Orjuela Martinez       1.00      1.00      1.00         2\n",
            "   Ximena Andrea Arias Contreras       1.00      1.00      1.00         2\n",
            " Yeimy Katherine Alarcón Almanza       0.00      0.00      0.00         1\n",
            "   Yuneidy Lorena Gutierrez Diaz       1.00      1.00      1.00         2\n",
            "\n",
            "                        accuracy                           0.88        42\n",
            "                       macro avg       0.84      0.87      0.85        42\n",
            "                    weighted avg       0.87      0.88      0.87        42\n",
            "\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparación - Conclusión**\n",
        "\n",
        "FaceNet muestra un rendimiento ligeramente superior a ArcFace en el conjunto de datos, pero las diferencias son pequeñas. Ambos modelos parecen ser adecuados para la tarea de reconocimiento facial, y la elección final podría depender de otros factores como la velocidad de inferencia, la facilidad de implementación y los requisitos específicos del caso de uso.\n",
        "\n",
        "Estos modelos utilizan biometría facial con deep learning, pero no se trata la red neuronal toda la imagen, solo se usa como entrada el área seleccionada durante la detección. Además, para mejorar la precisión de la verificación facial, se puede realizar un alineamiento facial"
      ],
      "metadata": {
        "id": "09FYERKJr9Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicciones con nueva imagen"
      ],
      "metadata": {
        "id": "qvkHFWGPSaPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_facenet_embedding_local(img_path):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    face_tensor = mtcnn(img)\n",
        "    if face_tensor is not None:\n",
        "        face_tensor = face_tensor.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            emb = facenet(face_tensor)\n",
        "        return emb.cpu().numpy().flatten()\n",
        "    return None"
      ],
      "metadata": {
        "id": "jmR0jw1R1F4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arcface_embedding_from_link(link):\n",
        "    file_id = extraer_id_enlace(link)\n",
        "    if file_id is None:\n",
        "        return None\n",
        "    try:\n",
        "        img = np.array(cargar_imagen_desde_drive(file_id))\n",
        "        faces = arcface_app.get(img)\n",
        "        if len(faces) > 0:\n",
        "            return faces[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error ArcFace con {link}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "huJgGXKIO5oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def predict_identity(image_input, embedding_fn, train_db, threshold=0.5):\n",
        "    emb = embedding_fn(image_input)\n",
        "    if emb is None:\n",
        "        return \"No se pudo extraer embedding\"\n",
        "    centroids = {name: np.mean(embs, axis=0) for name, embs in train_db.items()}\n",
        "    sims = {name: cosine_similarity([emb], [centroid])[0][0] for name, centroid in centroids.items()}\n",
        "    best_match = max(sims, key=sims.get)\n",
        "    best_score = sims[best_match]\n",
        "    if best_score >= threshold:\n",
        "        return best_match\n",
        "    else:\n",
        "        return \"Desconocido\""
      ],
      "metadata": {
        "id": "B-X-n1XPO_mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_imagen = \"/content/drive/MyDrive/1748222062989.jpg\"  # Cambia por la ruta de tu imagen\n",
        "prediccion = predict_identity(ruta_imagen, get_facenet_embedding_local, facenet_train_db, threshold=0.5)\n",
        "print(f\"Predicción FaceNet: {prediccion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TelNqgF5PDFc",
        "outputId": "23f9021f-04fe-44d1-a187-4e0075d913e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicción FaceNet: Yuneidy Lorena Gutierrez Diaz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enlace_imagen = 'https://drive.google.com/file/d/1dLuaxe6Q9jepAJZY7lpFS8YGZYU294AD/view?usp=sharing'  # Cambia este enlace# Cambia por tu enlace\n",
        "prediccion = predict_identity(enlace_imagen, get_arcface_embedding_from_link, arcface_train_db, threshold=0.5)\n",
        "print(f\"Predicción ArcFace: {prediccion}\")"
      ],
      "metadata": {
        "id": "ijGEnShY1mJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983d357d-f29c-40bb-ef01-8928d166c28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicción ArcFace: JUAN FELIPE HERRERA RIANO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **R5: Reflexión Final – Limitaciones, Errores Frecuentes y Aspectos Éticos**\n",
        "\n",
        "* **Limitaciones Técnicas:**\n",
        "\n",
        "Cantidad de imágenes por persona: Algunas clases tienen solo 2 o 3 fotos, lo que limita la generalización del modelo.\n",
        "\n",
        "Condiciones variables: Cambios en iluminación, expresión o ángulo afectan la calidad del embedding.\n",
        "\n",
        "Dependencia de internet: La descarga de imágenes desde Drive introduce latencias y posibles errores si el enlace está roto.\n",
        "\n",
        "* **Errores Frecuentes:**\n",
        "\n",
        "Falsos positivos: Algunos estudiantes pueden ser mal identificados si sus rostros son similares o la imagen está mal tomada.\n",
        "\n",
        "Embeddings nulos: Algunas imágenes no permiten detectar rostros (por baja calidad, mal encuadre o error de red).\n",
        "\n",
        "Overfitting con pocas imágenes: Al tener una sola imagen por persona en entrenamiento, el modelo puede ajustarse demasiado a esa pose o condición específica.\n",
        "\n",
        "* **Aspectos Éticos:**\n",
        "\n",
        "Privacidad y consentimiento: Es fundamental que los estudiantes hayan dado consentimiento informado para que sus imágenes sean usadas con fines académicos.\n",
        "\n",
        "Seguridad de datos sensibles: Al tratar con imágenes faciales, se deben tomar medidas para proteger los datos y evitar usos indebidos.\n",
        "\n",
        "Sesgo algorítmico: Si hay sesgo en la recolección de imágenes (por ejemplo, más imágenes de ciertos grupos demográficos), el modelo puede tener mejor rendimiento para ciertos estudiantes que otros.\n",
        "\n",
        "Uso responsable: El sistema debe ser usado solo para propósitos educativos o administrativos con una política clara de uso y acceso.\n",
        "\n"
      ],
      "metadata": {
        "id": "u_ckQ0y-Sflf"
      }
    }
  ]
}